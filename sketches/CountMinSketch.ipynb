{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import AccumulatorParam\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global accumulator\n",
    "class MatrixAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return np.zeros(value.shape)\n",
    "    def addInPlace(self,val1,val2):\n",
    "        val1+= val2 \n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hash function   \n",
    "def hash_Md5(item,depth,width):\n",
    "    res = []\n",
    "    md5 = hashlib.md5(str(hash(item)).encode('utf-8'))\n",
    "    for i in range(depth):\n",
    "        md5.update(str(i).encode('utf-8'))\n",
    "        hashcode = int(md5.hexdigest(), 16) % width\n",
    "        res.append([i,hashcode])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountMinSKetch():\n",
    "    def __init__(self, delta, epsilon):\n",
    "        \n",
    "        if delta <= 0 or delta >= 1:\n",
    "            raise ValueError(\"delta must be between 0 and 1\")\n",
    "        if epsilon <= 0 or epsilon >= 1:\n",
    "            raise ValueError(\"epsilon must be between 0 and 1\")\n",
    "        \n",
    "        self.__name__ = \"CountMinSketch\"     \n",
    "        self.width = int(np.ceil(np.exp(1) / epsilon))\n",
    "        self.depth = int(np.ceil(np.log(1 / delta)))\n",
    "        self.count_table = sc.accumulator(np.zeros((self.depth, self.width)),MatrixAccumulatorParam())\n",
    "        \n",
    "    def getWidth(self):\n",
    "        return self.width\n",
    "    \n",
    "    def getDepth(self):\n",
    "        return self.depth\n",
    "    \n",
    "    def getCount(self):\n",
    "        return self.count_table\n",
    "    \n",
    "    def increment(self, file):\n",
    "        \n",
    "        depth = self.depth\n",
    "        width = self.width\n",
    "        count_table = self.count_table\n",
    "        \n",
    "        def addWord(item, depth, width):\n",
    "            hash_res = hash_Md5(item, depth, width)\n",
    "            for el in hash_res:\n",
    "                add_table = np.zeros((depth, width))\n",
    "                hash_num = el[0]\n",
    "                bucket = el[1]\n",
    "                add_table[hash_num][bucket] = 1\n",
    "                nonlocal count_table\n",
    "                count_table += add_table\n",
    "    \n",
    "        lines = sc.textFile(file).flatMap(lambda line: line.split(\" \"))\\\n",
    "                  .foreach(lambda word: addWord(word, depth, width))\n",
    "      \n",
    "    \n",
    "    def merge(self, cms):\n",
    "        if self.__name__ != cms.__name__:\n",
    "            raise Exception(\"Unable to merge!\")\n",
    "        \n",
    "        if self.depth != cms.depth:\n",
    "            raise Exception(\"Two count-min sketches need the same number of hash functions!\")\n",
    "            \n",
    "        if self.width != cms.width:\n",
    "            raise Exception(\"Two count-min sketches need the same number of buckets for each hash function!\")\n",
    "            \n",
    "        self.count_table += cms.count_table.value\n",
    "        \n",
    "            \n",
    "    def estimator(self, file):\n",
    "        \n",
    "        width = self.width\n",
    "        depth = self.depth\n",
    "        count_table = self.count_table.value\n",
    "        \n",
    "        def get_word_estimation(word, depth, width, count_table):\n",
    "            hash_value = hash_Md5(word, depth, width)\n",
    "            word_count = []\n",
    "            for coordinate in hash_value:\n",
    "                count = count_table[coordinate[0]][coordinate[1]]\n",
    "                word_count.append([word, count])\n",
    "            return word_count\n",
    "        \n",
    "        lines = sc.textFile(file)\n",
    "        estimate = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                        .flatMap(lambda word:get_word_estimation(word,depth,width,count_table,))\\\n",
    "                        .reduceByKey(min)\n",
    "                        #.foreach(lambda word: get_word_estimation(word, depth, width, count_table))\n",
    "        \n",
    "        return estimate.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hash functions are: 3\n",
      "The number of buckets for each hash function are: 91\n",
      "\n",
      "\n",
      "Here is the count table after increment:\n",
      "[[  3.   1.   4.  10.   5.   9.   1.   4.   4.   6.   4.   6.   1.   5.\n",
      "    7.   6.  15.   2.   6.   2.   2.   3.   2.   0.  13.   2.   2.   3.\n",
      "   15.   6.   2.  13.   4.   1.  72.   2.  15.   1.   2.   3.   3.   5.\n",
      "    3.   9.   1.  10.   6.   4.   6.   2.   0.  14.   4.  10.   6.   3.\n",
      "    4.   4.   2.   4.   4.   4.   6.   2.   4.  15.  22.   5.   3.   2.\n",
      "    1.   6.   5.   9.   4.   5.   2.   1.  25.   2.   1.   4.   1.   2.\n",
      "    6.   9.   5.   4.   8.   5.   4.]\n",
      " [  2.   4.   2.   5.  11.   5.  17.   3.   2.   7.   4.  10.   4.   1.\n",
      "    5.   4.   9.   1.   3.   3.   1.   1.   4.   6.   8.   3.   6.   2.\n",
      "   11.   4.   4.   5.   9.   4.   2.   3.   7.   3.   3.   2.   2.   4.\n",
      "    5.   2.   1.   1.   7.   9.  10.   0.   3.   4.   0.   4.   2.  25.\n",
      "    2.   4.   3.   6.   0.   6.   0.   3.   1.   4.  16.   5.   7.   8.\n",
      "    1.   4.   6.   2.   1.   6.   2.  15.  72.   5.  16.  17.  24.   2.\n",
      "    4.   1.   3.   5.  12.   1.   7.]\n",
      " [  8.   2.   7.   1.   6.   4.   2.   3.  15.   3.   5.  10.   0.   3.\n",
      "    2.   2.   9.   0.  11.   5.   3.   1.   5.   4.   2.   1.   3.   8.\n",
      "    5.   3.   0.   0.   5.   4.  16.   1.   3.   7.   3.   2.   5.   9.\n",
      "    2.   2.  16.   1.   3.   7.   5.   4.  11.   5.   3.   2.   2.   3.\n",
      "   13.  13.  31.   4.   0.   3.  12.   2.   3.   7.   8.   5.   4.   2.\n",
      "    6.  17.   5.  11.   6.   3.   2.   0.   3.   1.   1.   6.   4.   3.\n",
      "   16.   4.   3.   3.  74.   7.   4.]]\n"
     ]
    }
   ],
   "source": [
    "# User defined input delta and epsilon here:\n",
    "# the file path should be changed accordingly\n",
    "delta =0.07\n",
    "epsilon = 0.03\n",
    "incrementFile = 'file:///usr/local/spark/README.md'\n",
    "\n",
    "cms = CountMinSKetch(delta,epsilon) \n",
    "print(\"The number of hash functions are:\", cms.getDepth())\n",
    "print(\"The number of buckets for each hash function are:\", cms.getWidth())\n",
    "print(\"\\n\")\n",
    "\n",
    "cms.increment(incrementFile)\n",
    "print(\"Here is the count table after increment:\")\n",
    "print(cms.getCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we merge the same count table by loading the same file.\n",
      " The count table should be doubled\n",
      "[[   6.    2.    8.   20.   10.   18.    2.    8.    8.   12.    8.   12.\n",
      "     2.   10.   14.   12.   30.    4.   12.    4.    4.    6.    4.    0.\n",
      "    26.    4.    4.    6.   30.   12.    4.   26.    8.    2.  144.    4.\n",
      "    30.    2.    4.    6.    6.   10.    6.   18.    2.   20.   12.    8.\n",
      "    12.    4.    0.   28.    8.   20.   12.    6.    8.    8.    4.    8.\n",
      "     8.    8.   12.    4.    8.   30.   44.   10.    6.    4.    2.   12.\n",
      "    10.   18.    8.   10.    4.    2.   50.    4.    2.    8.    2.    4.\n",
      "    12.   18.   10.    8.   16.   10.    8.]\n",
      " [   4.    8.    4.   10.   22.   10.   34.    6.    4.   14.    8.   20.\n",
      "     8.    2.   10.    8.   18.    2.    6.    6.    2.    2.    8.   12.\n",
      "    16.    6.   12.    4.   22.    8.    8.   10.   18.    8.    4.    6.\n",
      "    14.    6.    6.    4.    4.    8.   10.    4.    2.    2.   14.   18.\n",
      "    20.    0.    6.    8.    0.    8.    4.   50.    4.    8.    6.   12.\n",
      "     0.   12.    0.    6.    2.    8.   32.   10.   14.   16.    2.    8.\n",
      "    12.    4.    2.   12.    4.   30.  144.   10.   32.   34.   48.    4.\n",
      "     8.    2.    6.   10.   24.    2.   14.]\n",
      " [  16.    4.   14.    2.   12.    8.    4.    6.   30.    6.   10.   20.\n",
      "     0.    6.    4.    4.   18.    0.   22.   10.    6.    2.   10.    8.\n",
      "     4.    2.    6.   16.   10.    6.    0.    0.   10.    8.   32.    2.\n",
      "     6.   14.    6.    4.   10.   18.    4.    4.   32.    2.    6.   14.\n",
      "    10.    8.   22.   10.    6.    4.    4.    6.   26.   26.   62.    8.\n",
      "     0.    6.   24.    4.    6.   14.   16.   10.    8.    4.   12.   34.\n",
      "    10.   22.   12.    6.    4.    0.    6.    2.    2.   12.    8.    6.\n",
      "    32.    8.    6.    6.  148.   14.    8.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Now we merge the same count table by loading the same file.\\n The count table should be doubled\")\n",
    "\n",
    "cms2 = CountMinSKetch(delta,epsilon)\n",
    "cms2.increment(incrementFile)\n",
    "cms.merge(cms2)\n",
    "print(cms.getCount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now estimate the times of appearance for each word in given test file:\n",
      "\n",
      "[('implementation', 0.0), ('this', 6.0), ('is', 12.0), ('python.', 8.0), ('min', 2.0), ('for', 26.0), ('test', 2.0), ('spark', 8.0), ('file', 12.0), ('sketch', 0.0), ('count', 8.0), ('on', 14.0), ('using', 14.0), ('hello', 2.0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Now estimate the times of appearance for each word in given test file:\\n\")\n",
    "estimateFile = 'file:///usr/local/spark/test.txt'    \n",
    "res = cms.estimator(estimateFile)\n",
    "print(res)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
